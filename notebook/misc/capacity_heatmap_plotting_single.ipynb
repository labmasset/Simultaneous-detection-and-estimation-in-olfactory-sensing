{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib h5py scipy seaborn scikit-learn --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544cd03-8dac-4ef8-b750-c675dc178649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent \n",
    "# sys.path.append(str(project_##root))\n",
    "\n",
    "BASE_DIR = Path.cwd().parent  # go up one level\n",
    "raw_input_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"l1\" / \"slam\" / \"bernoulli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db82e4-bd88-4789-80a7-01eafe7be1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "# Import the analysis module\n",
    "from mirrored_langevin_rnn.utils.data_pipeline.threshold_sweep_analysis import (\n",
    "    load_threshold_batch_files,\n",
    "    plot_threshold_heatmap,\n",
    "    merge_threshold_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bddf70c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the experiment configuration, including file patterns and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a3b45-a037-46fd-824a-0ef801fb700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"bernoulli\" / \"affinity_dense_gamma\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"bernoulli\" / \"affinity_sparse_gamma_sparsity_0.10\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"bernoulli\" / \"affinity_sparse_binary_sparsity_0.10\"\n",
    "out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"kumaraswamy\" / \"affinity_dense_gamma\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"kumaraswamy\" / \"affinity_sparse_binary_sparsity_0.10\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"slam\" / \"auc\" / \"kumaraswamy\" / \"affinity_sparse_gamma_sparsity_0.10\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"poisson\" / \"rank\" / \"affinity_dense_gamma\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"poisson\" / \"rank\" / \"affinity_sparse_binary_sparsity_0.10\"\n",
    "# out_dir = BASE_DIR / \"data\" / \"threshold_sweep\" / \"poisson\" / \"rank\" / \"affinity_sparse_gamma_sparsity_0.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATTERN = \"slam_auc_bernoulli_threshold_results_batch*.h5\"\n",
    "FILE_PATTERN = \"slam_auc_kumaraswamy_threshold_results_batch*.h5\"\n",
    "# FILE_PATTERN = \"poisson_rank_binary_threshold_results_batch*.h5\"\n",
    "# FILE_PATTERN = \"poisson_rank_threshold_results_batch*.h5\"\n",
    "\n",
    "BATCH_SIZE = 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0d3a4-e3c1-4b4c-b0e6-c5d6d7c94e3e",
   "metadata": {},
   "source": [
    "## Load and Process Threshold Sweep Data\n",
    "\n",
    "First, we'll load all the batch files, align them into a common grid, and merge them into a complete grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77389391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first file matching our pattern for inspection\n",
    "pattern_files = sorted(out_dir.glob(FILE_PATTERN))\n",
    "if pattern_files:\n",
    "    first_file = pattern_files[0]\n",
    "    print(f\"Inspecting first file: {first_file.name}\")\n",
    "    with h5py.File(first_file, \"r\") as f:\n",
    "        print(f.keys())\n",
    "        print(f.attrs.keys())\n",
    "        grid = f[\"grid\"]\n",
    "        print(f\"Grid shape: {grid.shape}\")\n",
    "        print(f\"Grid values: {grid}\") \n",
    "        grid_np = np.array(grid)\n",
    "        print(\"Grid as numpy array:\")\n",
    "        print(grid_np)\n",
    "        # n_sens_values = f[\"n_sens_values\"][:]\n",
    "        # n_odor_values = f[\"n_odor_values\n",
    "else:\n",
    "    print(f\"No files found matching pattern: {FILE_PATTERN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fefaa-373e-4e26-a9c4-8b3de69fb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all batch files and align them to a common grid\n",
    "grids, files, n_sens_values, n_odor_values = load_threshold_batch_files(\n",
    "    out_dir, \n",
    "    pattern=FILE_PATTERN\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(files)} batch files\")\n",
    "print(f\"Grid shape: {grids.shape}\")\n",
    "print(f\"nSens values: {n_sens_values}\")\n",
    "print(f\"nOdor values: {n_odor_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec118b9e-2703-46b5-a4a1-3f72dab4d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the grids into a complete grid using the first valid value at each position\n",
    "merged_grid, merged_path = merge_threshold_batches(\n",
    "    out_dir, \n",
    "    pattern=FILE_PATTERN,\n",
    "    output_file=\"threshold_results_merged.h5\",\n",
    "    merge_method=\"first_valid\"  # Use first valid value (alternatives: \"max\", \"min\", \"mean\")\n",
    ")\n",
    "\n",
    "print(f\"Merged grid shape: {merged_grid.shape}\")\n",
    "print(f\"Saved to: {merged_path}\")\n",
    "\n",
    "# Count non-NaN values to check grid completeness\n",
    "non_nan_count = np.count_nonzero(~np.isnan(merged_grid))\n",
    "total_cells = merged_grid.size\n",
    "print(f\"Grid completeness: {non_nan_count}/{total_cells} cells filled ({non_nan_count/total_cells:.1%})\")\n",
    "\n",
    "# Print a small sample of the grid\n",
    "print(\"\\nSample of merged grid (first 5x5 section):\")\n",
    "print(merged_grid[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3b4a2-3a8e-4d6c-8a1f-8a7b3b5d7a3c",
   "metadata": {},
   "source": [
    "## Visualize the Merged Threshold Sweep Results\n",
    "\n",
    "Now we'll create a heatmap visualization of the merged grid with contour lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ba48c-6ace-47a7-a27d-d8ffe320f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and plot the heatmap\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "fig, ax = plot_threshold_heatmap(\n",
    "    merged_grid,\n",
    "    n_odor_values,\n",
    "    n_sens_values,\n",
    "    figsize=(7,6),\n",
    "    cmap=\"Blues\",\n",
    "    sigma=1,  # Gaussian smoothing parameter\n",
    "    contour_levels = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    # contour_levels=(10, 20, 30),\n",
    "    highlight_level=20,\n",
    "    annot=False\n",
    ")\n",
    "ax.set_xlim([1000, 16000])\n",
    "norm = Normalize(vmin=5, vmax=100)\n",
    "def colorbar_config(norm, fig_rank, ax_rank):\n",
    "    cbar_rank = fig_rank.colorbar(ax_rank.collections[0], ax=ax_rank, shrink=0.9, aspect=9)\n",
    "    cbar_rank.mappable.set_norm(norm)\n",
    "    cbar_rank.set_ticks([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "    cbar_rank.ax.tick_params(width=0)\n",
    "    cbar_rank.set_ticklabels([\"\", '20', \"\", '40', \"\", '60', \"\", '80', \"\", '100+'], fontsize=16)\n",
    "    cbar_rank.outline.set_linewidth(2)\n",
    "    return cbar_rank\n",
    "\n",
    "cbar_slam = colorbar_config(norm, fig, ax)\n",
    "def add_contour_lines_to_colorbar(colorbar, levels, norm, label_offset=0.02):\n",
    "    for level in levels:\n",
    "        # normalized_level = norm(level)\n",
    "        if level == 20:\n",
    "            colorbar.ax.axhline(level, xmin=0.75, xmax=1, color='yellow', linestyle='solid', linewidth=2)\n",
    "        else:\n",
    "            colorbar.ax.axhline(level, xmin=0.75, xmax=1, color='black', linestyle='solid', linewidth=2)\n",
    "contour_levels = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "add_contour_lines_to_colorbar(cbar_slam, contour_levels, norm)\n",
    "\n",
    "cbar_slam.ax.set_visible(False)\n",
    "\n",
    "plt.savefig(out_dir / \"threshold_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b15d242-f5bc-46d2-8ef3-f7c40f7f33ad",
   "metadata": {},
   "source": [
    "## Check for Missing Data\n",
    "\n",
    "Let's check if there are any NaN values in our merged grid and visualize where they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc5e46-35b0-41ba-a53a-cd2eba6bf47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the merged grid\n",
    "nan_mask = np.isnan(merged_grid)\n",
    "nan_count = np.sum(nan_mask)\n",
    "print(f\"Number of NaN values in merged grid: {nan_count} out of {merged_grid.size} ({nan_count/merged_grid.size:.1%})\")\n",
    "# Print indices where data is missing\n",
    "nan_indices = np.where(nan_mask)\n",
    "print(f\"Missing data at grid positions (row, col): {list(zip(nan_indices[0], nan_indices[1]))}\")\n",
    "\n",
    "if nan_count > 0:\n",
    "    # Visualize where NaN values are\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    X, Y = np.meshgrid(n_odor_values, n_sens_values)\n",
    "    plt.pcolormesh(X, Y, nan_mask, cmap=\"binary\", shading=\"auto\")\n",
    "    plt.colorbar(label=\"NaN present\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"nOdor\")\n",
    "    plt.ylabel(\"nSens\")\n",
    "    plt.title(\"Missing Data in Merged Grid\")\n",
    "    plt.savefig(out_dir / \"missing_data_mask.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5c9b6-c9a7-4a5a-b7c7-7b7a6e4e7a0f",
   "metadata": {},
   "source": [
    "## Load and Plot from the Merged Grid File\n",
    "\n",
    "This section demonstrates how to load a previously saved merged grid and create a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf092eb-48c7-470e-b4f9-e1b68f7d4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a grid from an HDF5 file\n",
    "def load_grid_from_file(file_path):\n",
    "    \"\"\"Load a grid and its axis values from an HDF5 file.\"\"\"\n",
    "    with h5py.File(file_path, \"r\") as f:\n",
    "        grid = f[\"grid\"][:]\n",
    "        n_odor_vals = list(np.asarray(f.attrs[\"nOdor_values\"], dtype=int))\n",
    "        n_sens_vals = list(np.asarray(f.attrs[\"nSens_values\"], dtype=int))\n",
    "    return grid, n_odor_vals, n_sens_vals\n",
    "\n",
    "# Load from the saved merged file\n",
    "grid_file = out_dir / \"threshold_results_merged.h5\"\n",
    "loaded_grid, loaded_n_odor, loaded_n_sens = load_grid_from_file(grid_file)\n",
    "\n",
    "print(f\"Loaded grid shape: {loaded_grid.shape}\")\n",
    "print(f\"nOdor values: {loaded_n_odor}\")\n",
    "print(f\"nSens values: {loaded_n_sens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9114e3-8fa6-4f65-875f-d94443d46667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loaded grid with different visualization parameters\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plot_threshold_heatmap(\n",
    "    loaded_grid,\n",
    "    loaded_n_odor,\n",
    "    loaded_n_sens,\n",
    "    cmap=\"viridis\",  # Different colormap to show variation\n",
    "    sigma=0.5,       # Less smoothing\n",
    "    contour_levels=(10, 15, 20, 25, 30),  # More contour levels\n",
    "    highlight_level=20\n",
    ")\n",
    "\n",
    "plt.title(\"Threshold Heatmap from Loaded File\", fontsize=16)\n",
    "plt.savefig(out_dir / \"threshold_heatmap_loaded.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for Missing SLURM Array Job Indices\n",
    "\n",
    "# This function uses NaN values in the merged grid to determine which \n",
    "# SLURM job array indices are missing, matching the actual experiment slicing\n",
    "\n",
    "def check_missing_job_indices(out_dir, pattern, batch_size):\n",
    "    \"\"\"\n",
    "    Check for missing SLURM array job indices by examining NaN values in the merged grid.\n",
    "    \"\"\"\n",
    "    # Load the merged grid\n",
    "    try:\n",
    "        merged_grid, _ = merge_threshold_batches(\n",
    "            out_dir, \n",
    "            pattern=pattern,\n",
    "            output_file=\"threshold_results_merged.h5\",\n",
    "            merge_method=\"first_valid\"\n",
    "        )\n",
    "        print(f\"Loaded merged grid with shape: {merged_grid.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading merged grid: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get actual parameter values from files\n",
    "    try:\n",
    "        _, _, n_sens_actual, n_odor_actual = load_threshold_batch_files(out_dir, pattern=pattern)\n",
    "        n_sens_list = list(n_sens_actual)\n",
    "        n_odor_list = list(n_odor_actual)\n",
    "        print(f\"Parameter space: {len(n_odor_list)} odor × {len(n_sens_list)} sensor values\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading parameter values: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Generate parameter combinations in experiment order\n",
    "    combos = [(o, s) for o in n_odor_list for s in n_sens_list]\n",
    "    total_combinations = len(combos)\n",
    "    expected_jobs = (total_combinations + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nMapping info:\")\n",
    "    print(f\"  Total combinations: {total_combinations}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Expected jobs: {expected_jobs} (indices 0-{expected_jobs-1})\")\n",
    "    \n",
    "    # Find missing combinations by checking NaN values\n",
    "    missing_combinations = []\n",
    "    for i, (odor_val, sens_val) in enumerate(combos):\n",
    "        try:\n",
    "            odor_idx = n_odor_list.index(odor_val)\n",
    "            sens_idx = n_sens_list.index(sens_val)\n",
    "            \n",
    "            if np.isnan(merged_grid[sens_idx, odor_idx]):\n",
    "                missing_combinations.append((i, odor_val, sens_val))\n",
    "        except ValueError:\n",
    "            missing_combinations.append((i, odor_val, sens_val))\n",
    "    \n",
    "    # Map missing combinations to job indices\n",
    "    missing_job_indices = set()\n",
    "    for combo_idx, _, _ in missing_combinations:\n",
    "        job_idx = combo_idx // batch_size\n",
    "        missing_job_indices.add(job_idx)\n",
    "    \n",
    "    missing_job_indices = sorted(missing_job_indices)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Missing parameter combinations: {len(missing_combinations)}\")\n",
    "    print(f\"  Missing job indices: {len(missing_job_indices)}\")\n",
    "    \n",
    "    if missing_combinations:\n",
    "        print(f\"\\nFirst 10 missing combinations (combo_idx, odor, sensor):\")\n",
    "        for combo_idx, odor_val, sens_val in missing_combinations[:10]:\n",
    "            job_idx = combo_idx // batch_size\n",
    "            pos_in_job = combo_idx % batch_size\n",
    "            print(f\"  {combo_idx:3d}: ({odor_val:5d}, {sens_val:3d}) → job {job_idx}, pos {pos_in_job}\")\n",
    "        if len(missing_combinations) > 10:\n",
    "            print(f\"  ... and {len(missing_combinations) - 10} more\")\n",
    "    \n",
    "    if missing_job_indices:\n",
    "        # Group consecutive indices\n",
    "        ranges = []\n",
    "        start = missing_job_indices[0]\n",
    "        end = start\n",
    "        \n",
    "        for idx in missing_job_indices[1:]:\n",
    "            if idx == end + 1:\n",
    "                end = idx\n",
    "            else:\n",
    "                if start == end:\n",
    "                    ranges.append(str(start))\n",
    "                else:\n",
    "                    ranges.append(f\"{start}-{end}\")\n",
    "                start = end = idx\n",
    "        \n",
    "        if start == end:\n",
    "            ranges.append(str(start))\n",
    "        else:\n",
    "            ranges.append(f\"{start}-{end}\")\n",
    "        \n",
    "        print(f\"\\nMissing job indices (ranges): {', '.join(ranges)}\")\n",
    "        print(f\"Missing job indices (list): {missing_job_indices}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No missing job indices - all combinations completed!\")\n",
    "    \n",
    "    # Grid statistics\n",
    "    total_cells = merged_grid.size\n",
    "    nan_cells = np.sum(np.isnan(merged_grid))\n",
    "    completeness = (total_cells - nan_cells) / total_cells * 100\n",
    "    \n",
    "    print(f\"\\nGrid completeness: {completeness:.1f}% ({total_cells - nan_cells}/{total_cells})\")\n",
    "    \n",
    "    return {\n",
    "        'missing_combinations': missing_combinations,\n",
    "        'missing_job_indices': missing_job_indices,\n",
    "        'total_combinations': total_combinations,\n",
    "        'expected_jobs': expected_jobs,\n",
    "        'grid_completeness': completeness\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "result = check_missing_job_indices(\n",
    "    out_dir, \n",
    "    pattern=FILE_PATTERN,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c0d3a4-e3c1-4b4c-b0e6-c5d6d7c94e3f",
   "metadata": {},
   "source": [
    "## Export Grid Data for Further Analysis\n",
    "\n",
    "This section shows how to export the grid data to other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fc007-785c-4dca-a3bc-2c29cbb85b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export grid to CSV for use in other tools\n",
    "def export_grid_to_csv(grid, n_odor_vals, n_sens_vals, output_path):\n",
    "    \"\"\"Export grid data to CSV format with row/column headers.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame with proper indices\n",
    "    df = pd.DataFrame(grid, index=n_sens_vals, columns=n_odor_vals)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Grid exported to {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "csv_path = out_dir / \"threshold_grid_merged.csv\"\n",
    "df = export_grid_to_csv(merged_grid, n_odor_values, n_sens_values, csv_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
